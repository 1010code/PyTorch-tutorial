# 激發函數(Activation Function)
激發函數也就是為了解決我們日常生活中不能用線性方程所概括的問題。我們假設, 女生長得越漂亮, 越多男生愛. 這就可以被當做一個線性問題. 但是如果我們假設這個場景是發生在校園裡. 校園裡的男生數是有限的, 女生再漂亮, 也不可能會有無窮多的男生喜歡她. 所以這就變成了一個非線性問題.

![](https://morvanzhou.github.io/static/results/ML-intro/active1.png)

`Y = Wx`， W 就是我們要求的參數 y 是預測值 x 是輸入值。用這個式子，我們很容易就能描述一個線性問題，因為W是一個固定的常數。但是我們該如何將線性轉為非線性呢？關鍵就在於激發函數，其實激發函數就是另外一個非線性函數。比如說relu, sigmoid, tanh。透過強行把原有的線性結果給扭曲了，使得輸出結果y也有了非線性的特徵。比如我使用了relu，此時Wx 的結果是1, y 還將是1, 不過Wx 為-1的時候, y 不再是-1, 而會是0。

你甚至可以創造自己的激勵函數來處理自己的問題，不過要確保的是這些激勵函數必須是可以微分的，因為在backpropagation 誤差反向傳遞的時候，只有這些可微分的激勵函數才能把誤差傳遞回去.

![](https://morvanzhou.github.io/static/results/ML-intro/active3.png)

## 激發函數的作用
是為了增加神經網絡模型的非線性。否則你想想，沒有激活函數的每層都相當於矩陣相乘。就算你疊加了若干層之後，無非還是個矩陣相乘罷了。所以你沒有非線性結構的話，根本就算不上什麼神經網絡。

激發函數是一個非線性的函數，普通的神經網路每一層的計算出來的都是一個線性的關係。如果要處理日常生活中複雜的問題是一個非線性問題，你不能用一條直線來描述他，模型曲線可能是彎彎曲曲。所以我們才需要採用神經網路當中的非線性畫的手段叫激發函數。所謂的激發函數就是每一層神經網路計算出來的結果，這個結果最後會再通過激發函數例如relu來變成另一個結果使得非線性化。透過這樣的非線性化手段，我們就能使神經網路變得更強大。

## Relu 優點
第一，採用sigmoid等函數，算激活函數時（指數運算），計算量大，反向傳播求誤差梯度時，求導涉及除法，計算量相對大，而採用Relu激活函數，整個過程的計算量節省很多。

第二，對於深層網絡，sigmoid函數反向傳播時，很容易就會出現梯度消失的情況。在sigmoid接近飽和區
時，變換太緩慢，導數趨於0，這種情況會造成信息丟失，從而無法完成深層網絡的訓練。

第三，Relu會使一部分神經元的輸出為0，這樣就造成了網絡的稀疏性，並且減少了參數的相互依存關係，緩解了過擬合問題的發生

